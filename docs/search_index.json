[["index.html", "è®ºæ–‡å†™ä½œå¬è¯¾ç¬”è®° Chapter1 å‰è¨€ 1.1 ç¬”è®°æ¥æº 1.2 è®ºæ–‡æ¡†æ¶", " è®ºæ–‡å†™ä½œå¬è¯¾ç¬”è®° Tan Jay 2022-09-15 Chapter1 å‰è¨€ è¿™æ˜¯bookdown1 2 3 çš„å°è¯•ç‰›åˆ€ã€‚ 1.1 ç¬”è®°æ¥æº å…¬ä¼—å·ï¼šè®¡ç®—æœºè§†è§‰è”ç›Ÿã€‚å®¡ç¨¿äººï¼šå¦‚ä½•å†™å¥½ä¸€ç¯‡å­¦æœ¯è®ºæ–‡?! ç½‘ç«™ï¼šæ·±åº¦ä¹‹çœ¼ã€‚è®ºæ–‡æŒ‡å¯¼ç³»åˆ—è¯¾ç¨‹ 1.2 è®ºæ–‡æ¡†æ¶ é¢˜ç›®/ä½œè€… æ‘˜è¦ 6 å¼•è¨€ 5 ç›¸å…³å·¥ä½œ 3 æå‡ºæ–¹æ³• 4 å®éªŒ 7 ç»“è®º 6 å‚è€ƒæ–‡çŒ® The bookdown book: https://bookdown.org/yihui/bookdown/ â†©ï¸ Rè¯­è¨€æ•™ç¨‹ï¼šhttps://www.math.pku.edu.cn/teachers/lidf/docs/Rbook/html/_Rbook/index.htmlâ†©ï¸ åŒ»å­¦ç»Ÿè®¡ç¬”è®°ä¸Rè¯­è¨€ï¼š https://wxhyihuan.github.io/MedicalStatisNotes/ GitHubä»“åº“ï¼š https://github.com/wxhyihuan/MedicalStatisNotesâ†©ï¸ "],["å¯¼è®º.html", "Chapter2 å¯¼è®º 2.1 æ€ç»´å¯¼å›¾ 2.2 ç§‘å­¦ç ”ç©¶çš„åŸºæœ¬æµç¨‹ 2.3 ç§‘å­¦çš„å¯»æ‰¾å’Œæ€»ç»“æ–‡çŒ® 2.4 å¯»æ‰¾ç ”ç©¶ç—›ç‚¹å’Œç ”ç©¶æ–¹å‘ 2.5 å¸¸ç”¨çš„æœºå™¨å­¦ä¹ ç ”ç©¶å¹³å° 2.6 è®ºæ–‡å†™ä½œå’Œç»˜å›¾å·¥å…· 2.7 å°å®‹è€å¸ˆçš„å»ºè®®", " Chapter2 å¯¼è®º 2.1 æ€ç»´å¯¼å›¾ 2.2 ç§‘å­¦ç ”ç©¶çš„åŸºæœ¬æµç¨‹ 2.2.1 ç§‘å­¦ç ”ç©¶çš„å®šä¹‰ å®šä¹‰ Research is â€œcreative and systematic work undertaken to increase the stock of knowledgeâ€. It involves the collection, organization, and analysis of information to increase understanding of a topic or issue. A research project may be an expansion on past work in the field. Research projects can be used to develop further knowledge on a topic, or for education. ç›®çš„ æ‰©å±•æˆ–æå‡ç›®å‰çš„çŸ¥è¯†æˆ–æŠ€æœ¯ ä¸ºåˆ›é€ å‘æ˜æ–°äº§å“å’Œæ–°æŠ€æœ¯æä¾›ç†è®ºä¾æ® æ‰‹æ®µ æ•°æ®é‡‡é›† æ•°æ®ç»„ç»‡ æ•°æ®åˆ†æ å®éªŒéªŒè¯ 2.2.2 ç ”ç©¶çš„åŸºæœ¬æµç¨‹ 1. äº†è§£è¡Œä¸šèƒŒæ™¯ ä¸å¯¼å¸ˆæˆ–è¡Œä¸šçš„å¸ˆå…„å¸ˆå§è¿›è¡Œå’¨è¯¢ æµè§ˆè¡Œä¸šç›¸å…³çš„å¯¼è®ºæ€§è´¨ä¹¦ç±æˆ–è§†é¢‘ 2. å­¦ä¹ åŸºç¡€çŸ¥è¯† æµè§ˆè¡Œä¸šæ ¸å¿ƒä¹¦ç±æˆ–è§†é¢‘ å¯¹ä¹¦ä¸Šçš„å…¸å‹æ¡ˆä¾‹è¿›è¡Œä»£ç å¤ç° 3. é˜…è¯»ç›¸å…³æ–‡çŒ® è°·æ­Œå­¦æœ¯ https://scholar.google.com ä¸­å›½çŸ¥ç½‘ https://www.cnki.net IEEE Xplore http://ieeexplore.ieee.org ACM Elsevier Springer 4. æ‰¾åˆ°ç ”ç©¶æ–¹å‘ åº”ç”¨æ–¹å‘ï¼ˆNLPï¼ŒCVï¼ŒHCIç­‰ï¼‰ æ‰¾åˆ°ç ”ç©¶ç—›ç‚¹ æŠ€æœ¯æ–¹å‘ï¼ˆé¢„å¤„ç†ç®—æ³•ï¼Œç‰¹å¾æå–ç®—æ³•ï¼Œåˆ†ç±»å™¨ç­‰ï¼‰ 5. æå‡ºç ”ç©¶è®¾æƒ³ ç†è®ºè°ƒç ”å’Œå…¬å¼æ¨å¯¼ æŠ€æœ¯å®ç°å¯è¡Œæ€§åˆ†æ å®éªŒæ•°æ®åº“ 6. å®ç°è®¾æƒ³ç³»ç»Ÿå’ŒéªŒè¯ç³»ç»Ÿ å¼€æºä»£ç è°ƒç ” ç¼–ç¨‹å’Œè°ƒè¯• å®éªŒè®¾è®¡ æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç† æœºå™¨å­¦ä¹ æ¨¡å‹æ­å»º ç¯å¢ƒï¼Œæ•°æ®äº¤äº’å’Œå¯è§†åŒ–æ¨¡å—æ­å»º è®­ç»ƒå’ŒéªŒè¯ä»£ç ç¼–å†™ æµ‹è¯•ä»£ç ç¼–å†™ 7. æ’°å†™è®ºæ–‡ é¢˜ç›®/ä½œè€…/æ‘˜è¦ 6 å¼•è¨€ 5 ç›¸å…³å·¥ä½œ 3 æå‡ºæ–¹æ³• 4 å®éªŒ 7 ç»“è®º 6 å‚è€ƒæ–‡çŒ® 8. æ€»ç»“å›é¡¾ æ˜¯å¦æœ‰ç›¸ä¼¼è®ºæ–‡æœ€è¿‘å‘è¡¨ è®ºæ–‡çš„ç†è®ºå’Œå…¬å¼æ˜¯å¦ä»£ç ä¸€è‡´ å®éªŒç»“æœå¤ç° ä»£ç æˆ–æ•°æ®åº“å¼€æº 2.3 ç§‘å­¦çš„å¯»æ‰¾å’Œæ€»ç»“æ–‡çŒ® ç§‘å­¦æ–‡çŒ®æ˜¯æŒ‡é€šè¿‡ä¸€å®šçš„æ–¹æ³•å’Œæ‰‹æ®µã€è¿ç”¨ä¸€å®šçš„æ„ä¹‰è¡¨è¾¾å’Œè®°å½•ä½“ç³»ï¼Œè®°å½•åœ¨ä¸€å®šè½½ä½“çš„æœ‰å†å²ä»·å€¼å’Œç ”ç©¶ä»·å€¼çš„çŸ¥è¯†ã€‚ æœŸåˆŠï¼Œä¼šè®®å’Œå­¦æœ¯è®ºæ–‡ å‡†ç¡®ä¸“ä¸šçš„ç™¾ç§‘ç½‘ç«™ï¼ˆç»´åŸºç™¾ç§‘ï¼Œç™¾åº¦ç™¾ç§‘ç­‰ï¼‰ ä¸“ä¸šä¹¦ç±ï¼ˆæ•™ç§‘ä¹¦ã€ä¸“è‘—ç­‰ï¼‰ æŠ¥çº¸ç­‰å…¶ä»–ä¸“ä¸šå‡†ç¡®çš„è®°å½• 2.3.1 ç§‘å­¦çš„å¯»æ‰¾æ–‡çŒ® 1. ç¡®å®šç ”ç©¶å…³é”®å­— æ‰€ç”¨çš„å…·ä½“æŠ€æœ¯ è‡ªç›‘ç£å­¦ä¹  å¼±ç›‘ç£å­¦ä¹  å‚…ç«‹å¶å˜æ¢ ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ åº”ç”¨åœºæ™¯ äººè„¸è¯†åˆ« å›¾åƒå»å™ª è¯­ä¹‰åˆ†å‰² åŒæ­¥å»ºå›¾ä¸å®šä½ å‘è¡¨æ—¶é—´å’Œä½œè€… 2. æœç´¢å¼•æ“+å‡ºç‰ˆå•† è°·æ­Œå­¦æœ¯ https://scholar.google.comï¼ˆçˆ¬å¢™ï¼›å­¦ä¹ é«˜çº§æ£€ç´¢ï¼‰ ä¸­å›½çŸ¥ç½‘ https://www.cnki.net IEEE Xplore http://ieeexplore.ieee.org ACM Elsevier Springer 3. ä»€ä¹ˆè®ºæ–‡å€¼å¾—è¯»ï¼Ÿ é¡¶çº§æœŸåˆŠä¼šè®® é«˜å¼•ç”¨ æ¥è‡ªäºè‘—åå­¦è€…/ç ”ç©¶ç»„ 4. å¦‚ä½•å¯»æ‰¾é‡è¦çš„æœŸåˆŠä¼šè®®ï¼Ÿ äº†è§£æ‰€åœ¨ç ”ç©¶é¢†åŸŸçš„é‡è¦æœŸåˆŠå’Œä¼šè®® CCF https://www.ccf.org.cn/ ä¸­ç§‘é™¢åˆ†åŒº é‡è¦æœŸåˆŠ æœºå™¨å­¦ä¹ : IEEE PAMIï¼ŒIEEE TNNLSï¼Œ Machine Learningç­‰ è®¡ç®—æœºè§†è§‰:IEEE TIPï¼ŒIJCVç­‰ è¯­éŸ³å¤„ç†:IEEE TASLPï¼ŒACM SLPç­‰ å…¶ä»–:IEEE Trans on Cyberneticsï¼ŒIEEE TFSï¼ŒIEEE TACï¼ŒIEEE TMIï¼ŒIEEE Trans on Roboticsï¼ŒPattern Recognitionç­‰ 2.3.2 ç§‘å­¦çš„æ€»ç»“æ–‡çŒ® 1. é˜…è¯»æ‘˜è¦ è®ºæ–‡ç ”ç©¶ç›®çš„ï¼ˆæ˜¯å¦ä¸è‡ªå·±çš„ç ”ç©¶æ–¹å‘ä¸€è‡´ï¼‰ è®ºæ–‡æ‰€ç”¨çš„æ–¹æ³•ï¼ˆæ˜¯å¦å¯ä»¥æ”¹è¿›æˆ–åº”ç”¨åˆ°æˆ‘çš„ä»»åŠ¡ä¸­ï¼‰ è®ºæ–‡æ–¹æ³•çš„æ•ˆæœï¼ˆè¯¥æ–¹æ³•æ˜¯å¦å€¼å¾—æ›´æ·±ä¸€æ­¥çš„ç ”ç©¶ï¼‰ ä¾‹å­: The three-dimensional (3-D) path planning of unmanned aerial vehicles (UAV) is a multi-objective optimization problem. It aims to find a smooth, flyable, and optimal path from starting point to the target point in a complex environment.ï¼ˆç ”ç©¶ç›®çš„ï¼‰ Traditional algorithms have difficulty ensuring the optimal path when faced with multiple objectives and complex cost functions. In this work, an improved butterfly optimization algorithm (IBOA) based on the virtual center butterfly (VCB) and Neighborhood dimension perturbation learning (NDPL) is proposed to solve the problem.ï¼ˆç ”ç©¶æ–¹æ³•ï¼‰ Since BOA uses pairwise interactions between two random butterflies to perform the exploration. It makes the algorithm prone to miss the optimal solution, resulting in insufficient exploration capability. A novel VCB strategy was introduced into BOA to improve the exploration capability of the algorithm by creating attraction and repulsion effects on butterflies during the exploration phase. Meanwhile, the other individuals move toward the best individual in the exploitation phase, and if the best individual falls into the local extremes, it leads to premature convergence of the algorithm with low accuracy. A new NDPL is proposed, which constructs a neighborhood matrix after the BOA search is finished, then performs dimensional learning for each individual. The simulation experimental results in three scenarios show that the IBOA can acquire an effective and feasible route successfully, and its performance is superior to the other six algorithms.ï¼ˆæ–¹æ³•æ•ˆæœï¼‰ ç¬”è®° ç ”ç©¶ç›®çš„:å¤æ‚ç¯å¢ƒä¸­æ‰¾åˆ°å¯è¡Œçš„ã€å¯é£çš„ã€æœ€ä¼˜çš„è·¯å¾„ã€‚ ç ”ç©¶æ–¹æ³•:an improved butterfly optimization algorithm (IBOA)ã€‚ æ–¹æ³•æ•ˆæœ:è¯¥IBOAæ–¹æ³•å¯ä»¥æˆåŠŸè·å¾—æœ‰æ•ˆå¯è¡Œçš„è·¯å¾„ï¼Œå¹¶ä¸”æ¯”å…¶ä»–6ç§æ–¹æ³•å¥½ã€‚ 2. é˜…è¯»è®ºæ–‡ è®ºæ–‡ç ”ç©¶é¢†åŸŸ è®ºæ–‡æ‰€ç”¨æ–¹æ³•:å…·ä½“æµç¨‹ï¼ˆæ¡†æ¶ï¼‰ï¼Œæ ¸å¿ƒæŠ€æœ¯ï¼Œåˆ›æ–°ç‚¹ï¼ˆç‚¹ç›ä¹‹ç¬”ï¼‰ï¼Œå±€é™æ€§ è®ºæ–‡å®éªŒç»“æœ:åº”ç”¨åœºæ™¯ï¼Œæ•°æ®åº“ï¼Œå®éªŒè®¾è®¡ï¼Œå®éªŒç»“æœ è®ºæ–‡çš„ç»“è®º ç¬”è®° é¢˜ç›® ç ”ç©¶é¢†åŸŸçš„ç—›ç‚¹åŠç°çŠ¶ æ–¹æ³•ï¼Œé‡è¦ç»†èŠ‚ï¼Œåˆ›æ–°ç‚¹ å®éªŒç»“æœåŠç»“è®º ğŸ’¡åšç¬”è®°æ˜¯éå¸¸æœ‰å¿…è¦çš„ä¸€æ­¥ï¼Œç›¸å½“äºè‡ªåˆ¶ä¸€ä»½ç´¢å¼•ï¼Œæ—¥åå¿˜è®°äº†è¿˜èƒ½å¿«é€Ÿæ‹¿èµ·ã€‚ 2.4 å¯»æ‰¾ç ”ç©¶ç—›ç‚¹å’Œç ”ç©¶æ–¹å‘ 2.4.1 å¯»æ‰¾ç ”ç©¶ç—›ç‚¹ ç ”ç©¶ç—›ç‚¹çš„ç±»åˆ« ç ”ç©¶é¢†åŸŸå†…æ— äººåº”ç”¨è¿‡æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•å­˜åœ¨æœºå™¨å­¦ä¹ å¯ä»¥è§£å†³çš„é—®é¢˜ã€‚ ç ”ç©¶é¢†åŸŸå†…ç›®å‰å·²æœ‰ä¸»æµæ–¹æ³•æ‰€å­˜åœ¨çš„ç¼ºé™·ã€‚ ç ”ç©¶é¢†åŸŸå†…ç¼ºå°‘å¼€æºæ•°æ®åº“æˆ–å¸¸ç”¨å·¥å…·ã€‚ ç ”ç©¶é¢†åŸŸå†…ç¼ºå°‘å¯¹æ¯”æ€§æˆ–ç»¼è¿°æ€§è®ºæ–‡ã€‚ å‘ç°ç—›ç‚¹åï¼Œè°ƒç ”ç ”ç©¶ç—›ç‚¹ å­˜åœ¨è¯¥ç—›ç‚¹çš„åŸå› ï¼ˆæœ‰åˆ©äºè§£å†³é—®é¢˜ï¼‰ è¯¥ç—›ç‚¹å¯¹äºç ”ç©¶é¢†åŸŸçš„å½±å“ï¼ˆæ— æ„ä¹‰å°±æ²¡å¿…è¦äº†ï¼‰ è§£å†³è¯¥ç—›ç‚¹çš„å¯è¡Œæ€§ï¼ˆç›®å‰æ¡ä»¶æ— æ³•è§£å†³çš„ä¹Ÿæ²¡å¿…è¦ï¼‰ è§£å†³è¯¥ç—›ç‚¹çš„æ”¶ç›Š 2.4.2 å¯»æ‰¾ç ”ç©¶æ–¹å‘ A. ç ”ç©¶é¢†åŸŸå†…æ— äººåº”ç”¨è¿‡æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•å­˜åœ¨æœºå™¨å­¦ä¹ å¯ä»¥è§£å†³çš„é—®é¢˜ã€‚ è‹¥å¯ä»¥ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•è§£å†³è¯¥ç—›ç‚¹ï¼Œç»™äºˆç›®å‰å·²æœ‰æ¨¡å‹æå‡ºä¸€ç§èƒ½è§£å†³è¯¥ç—›ç‚¹çš„æ–¹æ³•ã€‚ âœ… è‹¥ä¸å¯ä»¥ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•è§£å†³è¯¥ç—›ç‚¹ï¼Œç ”ç©¶ä¸èƒ½è§£å†³çš„åŸå› ï¼Œå¹¶é€šè¿‡å¤šç§æ–¹å¼éªŒè¯è¿™äº›åŸå› ã€‚ ä¾‹å­1 ç ”ç©¶ç—›ç‚¹ï¼Œåœ¨ç²¾ç¥å¥åº·ï¼ˆè‡ªé—­ç—‡ï¼‰æ–¹å‘æœªæœ‰ç›´æ¥åˆ©ç”¨CVæˆ–MLçš„æ–¹æ³• å¯è¡Œæ€§æ–¹é¢ï¼Œå¯¹é¢éƒ¨è¡¨æƒ…è¿›è¡Œåˆ†ææ¥æ£€æµ‹çš„ç ”ç©¶ã€‚ è§£å†³æ”¶ç›Šæ–¹é¢ï¼Œä¼ ç»Ÿæ–¹æ³•ï¼ˆçœ‹å¿ƒç†åŒ»ç”Ÿï¼‰é€Ÿåº¦æ…¢ï¼Œä»·æ ¼é«˜ï¼ŒåŒ»ç”Ÿæ°´å¹³å‚å·®ä¸é½ï¼›æœºå™¨å­¦ä¹ æ–¹æ³•é€Ÿåº¦å¿«ï¼Œä»·æ ¼ä¾¿å®œï¼Œå®¢è§‚ ä¾‹å­2 ç ”ç©¶ç—›ç‚¹ï¼Œåœ¨æ±½è½¦è½®èƒç”Ÿäº§é¢†åŸŸæœªæœ‰ç›´æ¥åˆ©ç”¨CVæˆ–MLçš„æ–¹æ³• å¯è¡Œæ€§æ–¹é¢ï¼Œå¯¹è½®èƒå›¾åƒè¿›è¡Œåˆ†ææ¥æ£€æµ‹çš„ç ”ç©¶ã€‚ è§£å†³æ”¶ç›Šæ–¹é¢ï¼Œä¼ ç»Ÿæ–¹æ³•ï¼ˆäººå·¥æ£€æµ‹ï¼‰é€Ÿåº¦æ…¢ï¼Œä»·æ ¼é«˜ï¼Œæ£€æµ‹å‡†ç¡®åº¦ä½ï¼ˆäººä¼šç–²æƒ«ï¼‰ï¼›æœºå™¨å­¦ä¹ æ–¹æ³•é€Ÿåº¦å¿«ï¼Œä»·æ ¼ä¾¿å®œï¼Œæ£€æµ‹å‡†ç¡®åº¦é«˜ B. ç ”ç©¶é¢†åŸŸå†…ç›®å‰å·²æœ‰ä¸»æµæ–¹æ³•æ‰€å­˜åœ¨çš„ç¼ºé™·ã€‚ å¾ˆå¤šæ–¹æ³•åªåœ¨ç‰¹å®šæ¡ä»¶ä¸‹æ•ˆæœå¥½ã€‚âœ… èƒ½è§£å†³çš„è¯ï¼Œé’ˆå¯¹è¯¥ç¼ºé™·æå‡ºè§£å†³æ–¹æ¡ˆ:æ–°çš„é¢„å¤„ç†ï¼Œæ–°çš„å­¦ä¹ ç®—æ³•ï¼Œæ–°çš„æ•°æ®æ‰©å¢ï¼Œå¼•å…¥å¤šæ¨¡æ€æ•°æ®ç­‰ã€‚ï¼ˆåªè¦èƒ½è§£å†³ï¼‰ æœªè§£å†³çš„è¯ï¼Œé’ˆå¯¹è¯¥ç¼ºé™·å¯¹æ¯”å¤šç§è§£å†³æ–¹æ¡ˆã€‚ ä¾‹å­1 é—®é¢˜ ç†æƒ³æ•°æ®åº“ä¸­è®­ç»ƒå‡ºæ¥çš„äººè„¸è¯†åˆ«æ¨¡å‹åœ¨å…‰ç…§å˜åŒ–å¤§çš„åœ°æ–¹æ£€æµ‹å‡†ç¡®åº¦ä¸ä½³ã€‚ æå‡ºä¸€ä¸ªé¢„å¤„ç†æ–¹æ³•:æ•°æ®æ‰©å¢ï¼Œåœ¨è®­ç»ƒæ—¶åŠ å…¥å…‰ç…§ä¸ä¸€çš„å›¾åƒã€‚ æå‡ºä¸€ä¸ªæ–°çš„CNNæ¨¡å‹ï¼ŒåŠ ä¸€ä¸ªæ”¯å¹²ä¸“é—¨ç”¨äºå»é™¤å…‰ç…§å½±å“ã€‚ ä¾‹å­2 é—®é¢˜ æŸé¡¶çº§å›¾åƒåˆ†ç±»ç³»ç»Ÿéš¾ä»¥åŒºåˆ†å¤§å°ç±»ä¼¼çš„é©´å’Œé©¬ã€‚ æå‡ºä¸€ä¸ªäºŒæ¬¡åˆ†ç±»æ–¹æ³•:åœ¨åŸç³»ç»Ÿåˆ†å‡ºæ¥çš„ç»“æœä¹‹åï¼Œå†è¿›è¡Œä¸€æ¬¡äºŒæ¬¡åˆ†ç±»ï¼Œç”¨äºåŒºåˆ†é©´å’Œé©¬ã€‚ æå‡ºä¸€ä¸ªæ–°çš„ç‰¹å¾æå–æ–¹æ³•:åœ¨åŸç³»ç»Ÿä¸­åŠ å…¥ä¸€ä¸ªç‰¹å¾æå–ï¼Œä¸“æ³¨äºé©´å’Œé©¬æ ¸å¿ƒåŒºåˆ«çš„ç‰¹å¾ã€‚ C. ç ”ç©¶é¢†åŸŸå†…ç¼ºå°‘å¼€æºæ•°æ®åº“æˆ–å¸¸ç”¨å·¥å…·ã€‚ å¼€å‘æ–¹ä¾¿ä½¿ç”¨ï¼Œç¨³å®šä¸”æ•ˆæœå°šå¯çš„ç ”ç©¶å·¥å…·ã€‚ é‡‡é›†å¹¶å»ºç«‹å¯å¼€æºä¸”ç¬¦åˆè¡Œä¸šæ ‡å‡†çš„æ•°æ®åº“ã€‚ D. ç ”ç©¶é¢†åŸŸå†…ç¼ºå°‘å¯¹æ¯”æ€§æˆ–ç»¼è¿°æ€§è®ºæ–‡ã€‚ å°†æœ¬è¡Œä¸šçš„ä¸»æµç®—æ³•å¤ç°ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®åº“ä¸Šè¿›è¡Œæµ‹è¯•å¯¹æ¯”ã€‚ æ’°å†™ç»¼è¿°æ€§è®ºæ–‡ã€‚ 2.5 å¸¸ç”¨çš„æœºå™¨å­¦ä¹ ç ”ç©¶å¹³å° MATLAB è¯­è¨€:Matlab ä¼˜åŠ¿:ä½¿ç”¨æ–¹ä¾¿ï¼Œåº“å‡½æ•°ä¸°å¯Œï¼Œå¯åœ¨å¤šä¸ªä¸åŒæ“ä½œç³»ç»Ÿä¸Šä½¿ç”¨ åŠ£åŠ¿:å…¶æ·±åº¦å­¦ä¹ åº“è¿è¡Œé€Ÿåº¦è¾ƒæ…¢ï¼Œä¸é€‚åˆè¿›è¡Œå¤§å‹æ·±åº¦å­¦ä¹ å®éªŒ Caffe è¯­è¨€:C++ ä¼˜åŠ¿:é€Ÿåº¦å¿« åŠ£åŠ¿:çµæ´»æ€§ä¸è¶³ï¼Œéš¾æ˜“ä¸Šæ‰‹ï¼Œç¯å¢ƒé…ç½®å¤æ‚ PyTorch è¯­è¨€:Python ä¼˜åŠ¿:ä½¿ç”¨æ–¹ä¾¿ï¼Œçµæ´»ï¼Œå‚è€ƒæ–‡æ¡£ä¸°å¯Œï¼Œé€‚åˆåˆå­¦è€…ç ”ç©¶ä½¿ç”¨ åŠ£åŠ¿:æ— æˆç†Ÿçš„å¯è§†åŒ–æ¥å£ï¼Œå¯¼å‡ºçš„æ¨¡å‹è¾ƒéš¾ç§»æ¤ TensorFlow è¯­è¨€:Python ä¼˜åŠ¿:å¯è§†åŒ–ç¨‹åº¦é«˜ï¼Œè¯´æ˜æ–‡æ¡£ä¸°å¯Œ åŠ£åŠ¿:åˆå­¦è€…ä½¿ç”¨è¾ƒä¸ºå¤æ‚ Karas è¯­è¨€:Python ä¼˜åŠ¿:æ–¹ä¾¿çš„APIæ¥å£ï¼Œæ¨¡å‹æ­å»ºå’Œå¯¼å‡ºç®€å• åŠ£åŠ¿:çµæ´»æ€§è¾ƒä½ å…¶ä»– Theano MXNET Torch å…¶ä»– 2.6 è®ºæ–‡å†™ä½œå’Œç»˜å›¾å·¥å…· 2.6.1 å†™ä½œå·¥å…· Latex å…±åŒåä½œå¹³å° https://www.overleaf.com 2.6.2 ç»˜å›¾å·¥å…· Microsoft Power Point Microsoft Power Visio OriginLab MATLAB Photoshop 2.7 å°å®‹è€å¸ˆçš„å»ºè®® å¦‚ä½•é«˜æ•ˆæ‰å®çš„åšç ”ç©¶ ä¸è¦ç€æ€¥ç›´æ¥è¿›å…¥é¡¹ç›®ï¼Œå…ˆå°†ä¹¦ä¸Šçš„æ¡ˆä¾‹ç”¨ä»£ç å¤ç°ã€‚ å…ˆåšè°ƒç ”ï¼Œå‘ç°å€¼å¾—ç ”ç©¶çš„ç—›ç‚¹ï¼Œå†å¼€å§‹ç ”ç©¶ã€‚ é˜…è¯»é¡¶çº§æœŸåˆŠå’Œä¼šè®®çš„è®ºæ–‡ï¼Œåšå¥½è®°å½•ã€‚ å…ˆåšç ”ç©¶ï¼Œå†å†™è®ºæ–‡ã€‚ ç”¨Latexå†™è®ºæ–‡ã€‚ "],["related-work.html", "Chapter3 ç›¸å…³å·¥ä½œå†™ä½œ 3.1 æ€ç»´å¯¼å›¾ 3.2 å‡†å¤‡å·¥ä½œ 3.3 æ’°å†™ç›¸å…³å·¥ä½œ 3.4 å®æˆ˜æ¼”ç»ƒ", " Chapter3 ç›¸å…³å·¥ä½œå†™ä½œ whatï¼Œwhyï¼Œwhenï¼Œhow 3.1 æ€ç»´å¯¼å›¾ 3.2 å‡†å¤‡å·¥ä½œ Related Work means that the overall goal is to describe the related research areas and to place your methodâ€™s contributions to the field in this context. 3.2.1 é˜…è¯»æ–‡çŒ® ç ”ç©¶æ–¹å‘çš„ç°çŠ¶å’ŒèƒŒæ™¯ è®ºæ–‡æ‰€è§£å†³çš„ç—›ç‚¹ è®ºæ–‡æå‡ºçš„æ–¹æ³•:ä¸»è¦æµç¨‹ï¼Œæ ¸å¿ƒç®—æ³•ï¼Œåˆ›æ–°ç‚¹ è®ºæ–‡çš„å®éªŒç»“æœå’Œç»“è®º:æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œå±€é™æ€§ 3.2.2 è®°å½•æ–‡çŒ® é¢˜ç›® èƒŒæ™¯ã€ç°çŠ¶ã€ç—›ç‚¹ æ–¹æ³•ã€é‡è¦ç»†èŠ‚ã€åˆ›æ–°ç‚¹ å®éªŒç»“æœã€ç»“è®º ğŸ’¡å°†ç¬”è®°è®°åœ¨åŒä¸€ä¸ªæ–‡æ¡£é‡Œ ä¾‹å­ 1 é¢˜ç›® Deep Residual Learning for Image Recognition ç°çŠ¶ The depth of representations is of central importance for many visual recognition tasks. ç—›ç‚¹ Deeper neural networks are more difficult to train because of the vanishing/exploding gradient. While several solutions can deal with this issue, the accuracy of existing approaches still degrades rapidly with the networks depth increasing. æ–¹æ³• Introducing a deep residual learning framework. åˆ›æ–°ç‚¹ The shortcut connections simply perform identity mapping. å®éªŒç»“æœåŠç»“è®º The results show that their approach allows deep network to be easier to train due to less complex strcuture and obtained the state-of-the-art performance on multiple image classification datasets ğŸ’¡åœ¨æ’°å†™ç›¸å…³å·¥ä½œç« èŠ‚å‰ï¼Œåœ¨é˜…è¯»ç ”ç©¶ç›¸å…³çš„æ–‡çŒ®æ—¶ï¼Œä»æ‰€ç”¨æŠ€æœ¯ï¼Œç ”ç©¶æ–¹å‘ç­‰è§’åº¦å°½é‡è¾¾åˆ°é¢é¢ä¿±åˆ°çš„é˜…è¯»å’Œç†è§£ã€‚åŒæ—¶ï¼Œåœ¨é˜…è¯»æ—¶ä¸€å®šè¦åšå¥½ç¬”è®°ï¼Œå¹¶æ ¹æ®æ–‡çŒ®ç±»å‹è¿›è¡Œåˆ†ç±»ã€‚è¿™æ ·å¯ä»¥ä½¿åœ¨æ’°å†™ç›¸å…³å·¥ä½œç« èŠ‚æ—¶ï¼Œé«˜æ•ˆï¼Œå‡†ç¡®ï¼Œå¿«é€Ÿã€‚ 3.3 æ’°å†™ç›¸å…³å·¥ä½œ 3.3.1 ç›®çš„ The overall goal is to describe the related research areas and to place your methodâ€™s contributions to the field in this context. By clearly describing previous work, you can better describe the current limitations and the need for new methodology. It also gives you an opportunity to demonstrate knowledge of the area and helps others relate your current work to other scientific areas. 3.3.2 åŸºæœ¬ç»“æ„ æ€»èµ·/æ¦‚æ‹¬æœ¬èŠ‚å†…å®¹ æ€»ç»“å„æ–‡çŒ® æ®µè½åˆ†ç±» æ¯ç¯‡æ–‡çŒ®æ€»ç»“ç»“æ„ ç¬¬ä¸€éƒ¨åˆ†:æ€»ç»“ç°æœ‰ç›¸å…³æ–¹æ³•/ç« èŠ‚çš„æƒ…å†µï¼ˆå¯çœç•¥ï¼‰ æ–¹æ³•1: 1-2å¥æ€»èµ· In the past decade, many related studies have been proposed and published. Since 1980, XXX (ç ”ç©¶é¢†åŸŸ) has been rapidly developed. æ€»ç»“ç°æœ‰ç›¸å…³å·¥ä½œçš„æƒ…å†µï¼Œå¯¹å·²æœ‰æ–¹æ³•è¿›è¡Œåˆ†ç±»æˆ–æ—¶åºæ’åˆ— åˆ†ç±» These approaches can be mainly categorized into three classes:1.XXX;2.XXX;3.XXX. æ—¶åº Early studies mainly focused on XXX (æ—©æœŸæ–¹æ³•æ¦‚è¿°), which XXX (æ—©æœŸæ–¹æ³•çš„ç¼ºé™·). During XXX (ä¸­æœŸæŸæ®µæ—¶é—´), most researchers XXX (ä¸­æœŸæ–¹æ³•æ¦‚è¿°), but XXX (ä¸­æœŸæ–¹æ³•ç¼ºé™·). In recent years, with the XXX (è¿‘æœŸæ–°æ–¹æ³•çš„æ¥æº) becoming XXX (è¿‘æœŸæ–¹æ³•æ¦‚è¿°), this type of techniques have been widely employed in XXX (æœ¬æ–‡ç ”ç©¶é¢†åŸŸ). æ–¹æ³•2: å¯¹æœ¬ç« èŠ‚ç»“æ„è¿›è¡Œæ¦‚è¿° This section presents XXX in XXX. Then, XXX and XXX are introduced in XXX , which are followed by XXX. ç¬¬äºŒéƒ¨åˆ†:åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†/æ®µè½è¯¦ç»†æ€»ç»“å„ç¯‡è®ºæ–‡ è§„åˆ’æ®µè½ æŒ‰æŠ€æœ¯è·¯çº¿åˆ†ç±»:æ¯ç§æŠ€æœ¯è·¯çº¿çš„è®ºæ–‡æ”¾åœ¨åˆ†/æ®µè½è¿›è¡Œæ€»ç»“ æŒ‰åº”ç”¨é¢†åŸŸåˆ†ç±»:æ¯ç§åº”ç”¨é¢†åŸŸçš„è®ºæ–‡æ”¾åœ¨ä¸€ä¸ªéƒ¨åˆ†/æ®µè½è¿›è¡Œæ€»ç»“ æŒ‰æŠ€æœ¯-åº”ç”¨åˆ†ç±»:ä¸æœ¬æ–‡æŠ€æœ¯ç›¸å…³çš„è®ºæ–‡æ”¾åœ¨ä¸€ä¸ªéƒ¨åˆ†/æ®µè½ ï¼Œä¸åº”ç”¨é¢†åŸŸç›¸å…³çš„è®ºæ–‡æ”¾åœ¨ä¸€ä¸ªéƒ¨åˆ†/æ®µè½ æŒ‰æŠ€æœ¯å‘å±•é˜¶æ®µåˆ†ç±»:æ¯ä¸ªé˜¶æ®µæ”¾åœ¨ä¸€ä¸ªéƒ¨åˆ†/æ®µè½ æ ¹æ®å…¶ä»–ç ”ç©¶é€»è¾‘æ¥åˆ†ç±» ä¸€äº›ä¾‹å­ æŒ‰æŠ€æœ¯åˆ†ç±» Albanie, Samuel, et al.Â â€œEmotion recognition in speech using cross-modal transfer in the wild.â€ Proceedings of the 26th ACM international conference on Multimedia. 2018. Video as stack of still images Video as spatial-temporal volumes Short and Long-Term Dynamics Multi-Stream Networks Motion Information Learning to Rank Videos æŒ‰é¢†åŸŸåˆ†ç±» Hu, Han, et al.Â â€œRelation networks for object detection.â€ Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. Object relation in post processing Sequential relation modeling Human centered scenarios Duplicate removal æŒ‰ç ”ç©¶é€»è¾‘å’ŒæŠ€æœ¯è·¯çº¿åˆ†ç±» Junior, J. C. S.J., et al.Â â€œFirst impressions: A survey on computer vision-based apparent personality trait analysis.â€ IEEE Transactions on Affective Computing The importance of first impressions in our lives How challenging and subjective can be apparent personality trait labeling/evaluation? Still Images Image sequence Audiovisual trait prediction Multimodal trait prediction æ–‡çŒ®æ€»ç»“æ ¼å¼ ç›®çš„--&gt;æ–¹æ³•--&gt;ç»†èŠ‚/åˆ›æ–°ç‚¹--&gt;ç»“æœ/ç»“è®º Author proposed a method that XXX (æ–¹æ³•ç»†èŠ‚/æ–¹æ³•æµç¨‹), which XXX (ç»“æœ/ç»“è®º). To deal with XXX (ç—›ç‚¹), XXX (æ–¹æ³•) was employed by sb, which XXX (è§£å†³ç—›ç‚¹çš„ç»†èŠ‚), and achieved XXX (ç»“æœ/ç»“è®º). ä¸€äº›ä¸¾ä¾‹ å¯¹æ¯ä¸€ç¯‡æ–‡çŒ®è¯¦ç»†åˆ†æ Biel et al.Â [17] studied personality impressions in conversational videos (vlogs) from facial expression analysis (ç›®çš„). In their work, a subset of the Youtube vlog dataset [57] is adopted, as well as a facial expression model based on Facial Action Coding System (FACS) (æ–¹æ³•). The task of first impressions prediction is addressed using Support Vector Regression (SVR) combined with statistics of facial activity based on frame-by-frame estimates (ç»†èŠ‚1). Moreover, they analyzed what specific facial expressions are most prominent for modeling each of the different impressions (ç»†èŠ‚2) . Results show that extraversion is the trait showing the largest activity cue utilization, which is related to the evidence found in the literature that extraversion is typically easier to judge [58], [59] (ç»“æœ). Later, Aran and Gatica-Perez [60] studied the use of social media content as a domain to learn apparent personality traits, in particular extraversion, aiming to transfer the knowledge extracted from conversational videos to small group settings (ç›®çš„). Ridge Regression and SVM classifiers are combined with statistics extracted from weighted Motion Energy Images for the task of personality impressions prediction (æ–¹æ³•/ç»†èŠ‚). å¯¹æ–‡çŒ®è¿›è¡Œæ–¹æ³•åˆ†ç±» In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs)(ç›®çš„), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale(æ–¹æ³•/ç»†èŠ‚). An alternative to Multigrid is hierarchical basis pre-conditioning [44, 45], which relies on variables that represent residual vectors between two scales(æ–¹æ³•/ç»†èŠ‚). It has been shown [3, 44, 45] that these solvers converge much faster than standard solvers that are unaware of the residual nature of the solutions (ç»“æœç»“è®º). These methods suggest that a good reformulation or preconditioning can simplify the optimization. å¯¹æ–‡çŒ®è¿›è¡Œæ—¶åºåˆ†ç±» Practices and theories that lead to shortcut connections [2, 33, 48] have been studied for a long time(ç›®çš„). An early practice of training multi-laver perceptrons (MLPs) (æ–¹æ³•åç§°) is to add a linear layer connected from the network input to thy, output [33, 48] (æ–¹æ³•ç»†èŠ‚). In [43, 24], a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/exploding gradients. (æ–¹æ³•ç»†èŠ‚) The papers of [38, 37, 31, 46] propose methods for centering layer reponses, gradients, and propagated errors, implemented by shortcut connections. In [43], an inceptionâ€ layer is com-posed of a shortcut branch and a few deeper branches. (æ–¹æ³•ç»†èŠ‚) ç¬¬ä¸‰éƒ¨åˆ†:æ€»ç»“å·²æœ‰æ–¹æ³•çš„ç—›ç‚¹ï¼ˆä¸æœ¬æ–‡ç ”ç©¶æ–¹å‘ç›¸å…³ï¼‰ å†æ¬¡æ€»ç»“å·²æœ‰æ–¹æ³•çš„ç¼ºé™·å’Œç—›ç‚¹ æ€»ç»“æœ¬æ–‡æ‰€ä¾èµ–æŠ€æœ¯çš„ä¼˜åŠ¿ å†æ¬¡å¼ºè°ƒæœ¬æ–‡ç ”ç©¶åŠ¨æœº 3.4 å®æˆ˜æ¼”ç»ƒ é€šè¿‡ç¬”è®°å¿«é€Ÿå¼•å‡ºæ–‡çŒ® 1. å›é¡¾ç¬”è®° é¢˜ç›® Deep Residual Learning for Image Recognition ç°çŠ¶ The depth of representations is of central importance for many visual recognition tasks. ç—›ç‚¹ Deeper neural networks are more difficult to train because of the vanishing/exploding gradient. While several solutions can deal with this issue, the accuracy of existing approaches still degrades rapidly with the networks depth increasing. æ–¹æ³• Introducing a deep residual learning framework. åˆ›æ–°ç‚¹ The shortcut connections simply perform identity mapping. å®éªŒç»“æœåŠç»“è®º The results show that their approach allows deep network to be easier to train due to less complex strcuture and obtained the state-of-the-art performance on multiple image classification datasets 2. æç‚¼ç¬”è®° ç›®çš„ To deal with the exploding gradient problem during the training as well as enhancing the performance of deep models. æ–¹æ³• He et al.Â [1] proposed a deep residual network. ç»†èŠ‚ The shortcut connections simply perfrom identity mapping, and their outputs are added to the outputs of the stacked layers. ç»“æœ Obtained the state-of-the-art performance on multiple datasets and competitions. 3. æ•´ç†æˆæ®µ To deal with the exploding gradient problem during the training as well as enhancing the performance of deep models. He et al.Â [1] proposed a deep residual network, where the shortcut connections perform identity mapping, and their outputs are added to the outputs of the stacked layers. The results showed that this model obtained the state-of-the-art performance on multiple datasets and competitions. "],["approach.html", "Chapter4 æ–¹æ³•ç« èŠ‚å†™ä½œ 4.1 æ€ç»´å¯¼å›¾ 4.2 å‡†å¤‡å·¥ä½œ 4.3 æ’°å†™æ–¹æ³•ç« èŠ‚", " Chapter4 æ–¹æ³•ç« èŠ‚å†™ä½œ 4.1 æ€ç»´å¯¼å›¾ 4.2 å‡†å¤‡å·¥ä½œ The proposed approach The methods section of a research paper provides the information by which a studyâ€™s validity is judged. The method section answers two main questions: How was the data collected or generated? How was it analyzed? ç¡®å®šæ–¹æ³•æµç¨‹:ç¡®å®šå†™ä½œæ—¶çš„ç« èŠ‚åˆ†ç±» ç¡®å®šæ–¹æ³•åˆ›æ–°ç‚¹:ç¡®å®šå†™ä½œé£æ ¼ ç¡®å®šæ–¹æ³•ç»†èŠ‚:ç¡®å®šæ¯éƒ¨åˆ†ç¯‡å¹…é•¿çŸ­ï¼Œå®Œæˆå…¬å¼æ¨å¯¼ ğŸ’¡åœ¨æ’°å†™æ–¹æ³•ç« èŠ‚å‰ï¼Œå¿…é¡»è¦å·²ç»å®Œæˆç ”ç©¶ï¼Œè¿™æ ·æ‰å¯ä»¥ç¡®å®šå¥½æœ€ç»ˆçš„æ–¹æ³•æµç¨‹åŠç»†èŠ‚ã€‚åœ¨æ’°å†™ç« èŠ‚æ—¶ï¼Œè®ºæ–‡ä¸­çš„å†…å®¹ä¸€å®šè¦ä¸ä»£ç ç›¸ç¬¦åˆã€‚ 4.3 æ’°å†™æ–¹æ³•ç« èŠ‚ 4.3.1 åŸºæœ¬ç»“æ„ æ–¹æ³•ç« èŠ‚ä¸€èˆ¬é€šè¿‡ä¸‹åˆ—å‡ ç§æ–¹å¼è¿›è¡Œåˆ†èŠ‚ æŒ‰æ–¹æ³•æ­¥éª¤ æŒ‰æ–¹æ³•æ¨¡å— æŒ‰ç ”ç©¶é€»è¾‘ 4.3.2 ä¸»è¦å†…å®¹ æå‡ºæ–¹æ³•çš„ä¸»æµç¨‹ æ–¹æ³•ä¸»æµç¨‹ä¸€èˆ¬é™¤äº†åœ¨å¼•è¨€ä¸­ä»‹ç»å¤–ï¼Œä¸€èˆ¬è¿˜éœ€è¦åœ¨æ–¹æ³•ç« èŠ‚çš„å¼€å¤´è¿›è¡Œæ€»ç»“æ€§æ¦‚è¿°ï¼Œå¹¶è®²æ˜æ¯ä¸ªç« èŠ‚æ‰€ä»‹ç»æ–¹æ³•çš„éƒ¨åˆ†ã€‚ åœ¨æ–¹æ³•å¼€å¤´éƒ¨åˆ†ï¼Œä¸€èˆ¬æœ€å¥½åŠ ä¸Šæœ¬æ–‡æ–¹æ³•çš„æµç¨‹å›¾ æ–¹æ³•ç»†èŠ‚ åˆ›æ–°ç‚¹åŠæ ¸å¿ƒè´¡çŒ® å›¾è¡¨ ç ”ç©¶ç—›ç‚¹ 4.3.3 æ®µè½å†™ä½œ åœ¨æ¯ä¸€ç« èŠ‚ä¸­ï¼Œä¸è¦ä¸€ä¸Šæ¥å°±ç›´æ¥è¿›å…¥æ–¹æ³•ä»‹ç»ï¼Œå…ˆä»ç—›ç‚¹æˆ–è€…æå‡ºæ–¹æ³•æ‰€åŸºäºçš„åŸºæœ¬æŠ€æœ¯å¼€å§‹è¿™æ ·æœ‰æ˜“äºè¯»è€…ç†è§£ã€‚ å¤§éƒ¨åˆ†æ®µè½æœ€å¥½åœ¨æ®µè½å¼€å§‹å°±è¡¨æ˜æœ¬æ®µçš„ç›®çš„æˆ–è€…ä¸­å¿ƒæ€æƒ³ã€‚ "],["introduction.html", "Chapter5 å¼•è¨€å†™ä½œ 5.1 æ€ç»´å¯¼å›¾ 5.2 å‡†å¤‡å·¥ä½œ 5.3 æ’°å†™å¼•è¨€ 5.4 å®ä¾‹åˆ†æ 5.5 æ¨¡ç‰ˆ 5.6 å°å®‹è€å¸ˆå»ºè®®", " Chapter5 å¼•è¨€å†™ä½œ what why when how 5.1 æ€ç»´å¯¼å›¾ 5.2 å‡†å¤‡å·¥ä½œ The Introduction section clarifies the motivation for the work presented and prepares readers for the structure of the paper. 1. æ€»ç»“ç°æœ‰æ–‡çŒ® æ˜ç¡®ç›®å‰æ‰€ç ”ç©¶æ–¹å‘çš„ç°çŠ¶å’ŒèƒŒæ™¯ ç†è§£ç›®å‰æ‰€ç ”ç©¶æ–¹å‘çš„ä¸»æµæ–¹æ³• æ‰¾åˆ°ç›®å‰ä¸»æµæ–¹æ³•çš„é—®é¢˜å’Œç¼ºé™· 2. æ˜ç¡®ç ”ç©¶åŠ¨æœº ç¡®å®šç ”ç©¶å·¥ä½œçš„é‡å¿ƒ ç¡®å®šè®ºæ–‡å†™ä½œé‡å¿ƒ ç¡®å®šå®éªŒæ–¹å‘ 3. å®Œæˆç ”ç©¶å†…å®¹ æ˜ç¡®è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹ æ˜ç¡®æœ€ç»ˆä½¿ç”¨çš„æ–¹æ³•å’Œç ”ç©¶ç»†èŠ‚ ç¡®å®šè®ºæ–‡ä¸­éœ€è¦ä¹¦å†™çš„å†…å®¹ 4. å¾—åˆ°å®éªŒç»“æœ ç¡®å®šè®ºæ–‡å®éªŒå†…å®¹ æ ¹æ®å®éªŒç»“æœå¾—åˆ°è®ºæ–‡ç»“è®º å¯»æ‰¾æŠ•ç¨¿ç›®æ ‡æœŸåˆŠ/ä¼šè®® ğŸ’¡Introductionæ˜¯æ€»ç»“æ€§çš„ç« èŠ‚ï¼Œå‡ ä¹éœ€è¦åŒ…å«ç ”ç©¶å·¥ä½œçš„æ‰€æœ‰å†…å®¹ã€‚å› æ­¤åœ¨æ’°å†™å‰å»ºè®®å°†æ€»ç»“å¥½çš„æ–‡çŒ®å®Œæˆçš„ç ”ç©¶å†…å®¹å’Œå®éªŒç»“æœéƒ½ä»”ç»†å›é¡¾å‡ éï¼ŒåŠ æ·±å°è±¡ã€‚åœ¨æ’°å†™æ—¶ï¼Œå»ºè®®å°†è¿™äº›èµ„æ–™æ‘†åœ¨ä¸€æ—ï¼Œä»¥ä¾¿éšæ—¶æŸ¥é˜…ã€‚ 5.3 æ’°å†™å¼•è¨€ 5.3.1 åŸºæœ¬ç»“æ„ 5.3.2 ç ”ç©¶èƒŒæ™¯å’Œç—›ç‚¹ ç¬¬ä¸€å¥ï¼Œç ”ç©¶å¤§æ–¹å‘çš„å®šä¹‰ ç¬¬äºŒå¥ï¼Œç ”ç©¶æ–¹å‘æ„ä¹‰ ç¬¬ä¸‰å¥ï¼Œç ”ç©¶æ–¹å‘å½“å‰çš„å¤§è‡´ç°çŠ¶ ç¬¬å››å¥ï¼Œç—›ç‚¹ï¼ˆä¸æˆ‘ä»¬ç ”ç©¶å†…å®¹ç›¸å…³çš„ï¼Œæ–¹ä¾¿å¼•å‡ºä¸‹ä¸€æ®µï¼‰ 5.3.3 ç°æœ‰æ–¹æ³•çš„æ€»ç»“å½’çº³ 1. èµ·å§‹å¥:å¼•å‡ºç°æœ‰æ–¹æ³• ç”¨ä¸€å¥è¯å¼•å‡ºå½“å‰å·²ç»æå‡ºçš„å±äºæœ¬æ–‡ç ”ç©¶é¢†åŸŸçš„æ–¹æ³•: - (With XXX becoming XXX/Due to the recent advance in Ñ…Ñ…Ñ…), (a large number of attempts have been made/ a number of similar studies have been conducted) to XXX - XXX can be categorized into three fields: XXX, XXX and XXX - XXX is/are/becomes very popular in XXX field since XXX 2. æ¦‚è¿°ç›®å‰å·²æœ‰çš„ç»å…¸å·¥ä½œ æ—¶åºæ³•:æ ¹æ®å·²æœ‰æ–¹æ³•æå‡ºçš„æ—¶é—´å…ˆåé¡ºåºæˆ–å‘å±•é¡ºåºè¿›è¡Œé˜è¿°ã€‚ åˆ†ç±»æ³•:æ ¹æ®å·²æœ‰æ–¹æ³•æ‰€å±çš„ç±»åˆ«(å¦‚æ‰‹å·¥ç‰¹å¾/æ·±åº¦ç‰¹å¾ï¼Œä½¿ç”¨æ—¶åºä¿¡ æ¯/æœªä½¿ç”¨æ—¶åºä¿¡æ¯ç­‰)åˆ†åˆ«é˜è¿°ã€‚ ğŸ’¡åœ¨æ€»ç»“å„å·¥ä½œæ—¶ï¼Œ-èˆ¬åªéœ€è¦ç”¨1-2å¥è¯ï¼Œä¸å®œå¤ªé•¿(ä¸ç„¶ä¼šä¸Related Worké‡Œå†…å®¹é‡å¤)ã€‚æ€»ç»“æ—¶ï¼Œéœ€æ ¹æ®è®ºæ–‡çš„ç ”ç©¶å†…å®¹ï¼Œæ¦‚è¿°å„å·¥ä½œçš„ä¸»è¦(ç›¸å…³)æ–¹æ³•å’Œä¼˜ç¼ºç‚¹ã€‚ æ—¶åºæ³•ä¸¾ä¾‹ In particular, most of these studies [6- 10] firstly extract hand-crafted visual descriptors from face images and then feed them to pre-trained classifiers for prediction (ä¸€å¥è¯æ€»ç»“). Very early studies generally XXX (ç”¨XXXæ–¹æ³•) to obtain face representations, which XXX (è¿™äº›æ–¹æ³•çš„ç¼ºç‚¹). To solve these problems, Walker et al.Â [6] proposed a XXX method that (ä»‹ç»Walkerçš„æ–¹æ³•å’Œæ•ˆæœ). Alice et al.Â [7] XXX (ä»‹ç»Aliceçš„æ–¹æ³•å’Œæ•ˆæœ). However, both approaches XXX (1ä¸ªç»Walkerå’ŒAliceæ–¹æ³•çš„å±€é™æ€§). As a result, Luke et al.Â [8,9] and Yao et al.Â [10] specifically investigated XXX and (ä»‹ç»è¿‘æœŸæ›´å…ˆè¿›çš„æ–¹æ³•åŠå…¶å¦‚ä½•è§£å†³ä¹‹å‰æ–¹æ³•çš„é—®é¢˜). åˆ†ç±»æ³•ä¸¾ä¾‹ In particular, these studies [6-10] can be categorized into three categories: A-based approaches, B based approaches and C-based approaches (ä¸€å¥è¯æ€»ç»“). The systems proposed by Luke et al.Â [8,9] and Yao et al.Â [10] are the typical examples of A-based approaches, which XXX (ä»‹ç»Lukeå’ŒYaoçš„æ–¹æ³•å’Œæ•ˆæœ). Although A-based approach XXX (åŒ…æ‹¬ç”¨Aæ–¹æ³•çš„å¥½å¤„), they XXX (åŒ…æ‹¬ç”¨Aæ–¹æ³•çš„ç¼ºç‚¹). Thus, B-based approaches have been widely investigated. For example, Alice et al.Â [7] XXX (ä»‹ç»Aliceçš„æ–¹æ³•å’Œæ•ˆæœ). Besides, Walker et al.Â [6] extends C to face recognition tasks, by XXX (ä»‹ç»Walkerçš„æ–¹æ³•å’Œæ•ˆæœ) 3. æ€»ç»“ç›®å‰å·²æœ‰çš„ç»å…¸å·¥ä½œæ‰€å­˜åœ¨çš„é—®é¢˜ ä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸€éƒ¨åˆ†éœ€è¦æ€»ç»“ä¸æœ¬æ–‡å†…å®¹ç›¸å…³çš„é—®é¢˜ï¼Œå¹¶ä»¥æ­¤å¼•å‡ºæœ¬æ–‡çš„ Motivation. ä»¥äººè„¸è¯†åˆ«ä¸ºä¾‹: å¦‚æœä½ çš„è®ºæ–‡è®¾è®¡äº†ä¸€ç§åœ¨è¯†åˆ«ç‡æ›´é«˜ä½†é€Ÿåº¦å¾ˆæ…¢çš„æ–¹æ³•ï¼Œé‚£ä¹ˆæ­¤å¤„å°±è¦æ€»ç»“ç°æœ‰æ–¹æ³•å‡†ç¡®ç‡ä¸è¶³çš„é—®é¢˜å’Œå¯èƒ½ä¼šå¯¼è‡´çš„åæœã€‚å¦‚æœä½ çš„è®ºæ–‡è®¾è®¡äº†ä¸€ç§é€Ÿåº¦å¾ˆå¿«ä½†å‡†ç¡®åº¦æ— æ˜æ˜¾æå‡çš„æ–¹æ³•ï¼Œé‚£ä¹ˆè®ºæ–‡ä¸­å°±è¦æ€»ç»“ç°æœ‰æ–¹æ³•é€Ÿåº¦ä¸è¶³çš„é—®é¢˜å’Œå¯èƒ½ä¼šå¯¼è‡´çš„åæœã€‚ ä¸¾ä¾‹ While the aforementioned approaches already achieved good recognition performance (around 90% accuracy) [9] on the face dataset that collected under controlled environments [11]ï¼Œthey are still not able to provide reliable predictions on wild datasets [12-14] (ä¸€å¥è¯æ€»ç»“ç°æœ‰æ–¹æ³•ï¼Œä¸€èˆ¬å…ˆæ‰¬åæŠ‘). In addition, XXX (å¦‚æœä¸€å¥ä¸å¤Ÿï¼Œå¯ä»¥å†åŠ ä¸€å¥è¡¥å……ç°æœ‰æ–¹æ³•çš„ç¼ºç‚¹). This is because that existing hand-crafted visual descriptors are usually designed for general computer vision tasks without considering the face-specific information, which is crucial to face recognition tasks (å…·ä½“åˆ†æå½“å‰æ–¹æ³•çš„é—®é¢˜æ‰€åœ¨ã€‚ç”±äºæˆ‘ä»¬æ˜¯æå‡ºç”¨æ·±åº¦ç‰¹å¾æ¥åšäººè„¸è¯†åˆ«ï¼Œè‡ªç„¶æˆ‘ä»¬æ‰€è¦æå‡ºçš„é—®é¢˜å°±è¦é’ˆå¯¹äºå½“å‰çš„æ‰‹å·¥ç‰¹å¾). 5.3.4 æœ¬æ–‡ç ”ç©¶æ¦‚è¿° æœ¬æ–‡ç ”ç©¶ç»¼è¿°æ®µè½åŒ…å«äº†ç ”ç©¶ç›®çš„ï¼Œç ”ç©¶æ–¹æ³•å’Œå®éªŒè®¾è®¡ã€‚ï¼ˆå‘¼åº”ä¸Šä¸€æ®µæœ€åä¸€å¥è¯ï¼‰ æ ¹æ®ä¸Šæ®µæœ€åæ€»ç»“çš„ç°æœ‰æ–¹æ³•çš„ä¸»è¦é—®é¢˜ï¼Œæå‡ºæœ¬æ–‡çš„ç ”ç©¶ç›®çš„ã€‚ To address/solve/deal with XXX, this paper presents/proposes XXX In this paper, we aims to XXX by XXX As a consequence, this paper XXX æå‡ºå…·ä½“çš„è§£å†³æ–¹æ¡ˆã€‚ åŸºäºå“ªäº›å·²æœ‰çš„ç®—æ³• æå‡ºæ–¹æ³•çš„ä¸»è¦æµç¨‹åŠåŸç† æå‡ºæ–¹æ³•çš„åˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆ ä¸¾ä¾‹ In this paper, we extend Convolution Neural Networks (CNNs) to the face domain (åŸºäºCNNç®—æ³•). Specifically, the proposed approach starts with generating aligned face removing all background noises. Then, it feeds training faces to the proposed CNN model, paired with the corresponding labels. This way, the weights of the utilized CNN would be optimized, in order to correctly recognize the identity of the input face. In other words, the deep learned descriptors, which are generated by the well-trained CNN, are task -specific (æœ¬æ–‡æ–¹æ³•æ­¥éª¤å’ŒåŸç†). To the best of our knowledge, this is the first attempt to extract deep face descriptor for face recognition task (æœ¬æ–‡çš„åˆ›æ–°ç‚¹). æå‡ºéªŒè¯æ–¹æ¡ˆã€‚ éœ€è¦åšå“ªäº›å®éªŒ å®éªŒç›®çš„æ˜¯ä»€ä¹ˆ ç®€ç•¥å™è¿°å®éªŒç»“æœ ä¸¾ä¾‹ To evaluate the performance of the proposed deep learning approach on both controlled and wild conditions, this paper conducts experiments on XXX dataset and XXX dataset (éœ€è¦åšå“ªäº›å®éªŒå’Œå®éªŒç›®çš„). In addition, we also evaluate the learned deep feature on XXX face tasks (å¦‚æœè¿˜æœ‰é™¤ä¸»å®éªŒå¤–çš„å®éªŒï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ). The experimental results show that (ç®€è¦åˆ†æå®éªŒç»“æœ). æœ¬æ–‡çš„ä¸»è¦è´¡çŒ® æ–¹æ³•çš„åˆ›æ–°ç‚¹ å®éªŒç»“æœçš„å¤§å¹…æå‡ å¼€æºä»£ç  å¼€æºæ•°æ®åº“ 5.3.5 ç»“å°¾:æœ¬æ–‡è´¡çŒ®åŠè®ºæ–‡ç»“æ„ æœ¬æ–‡çš„ä¸»è¦è´¡çŒ® The main contributions of this study can be summarized as follows: This paper proposes the first deep learning-based face recognition system, which learns task-specific face descriptors rather than hand-crafted general visual descriptors (æ–¹æ³•å’Œåˆ›æ–°). The proposed approach achieved the state-of-the-art XXX performance on XXX dataset and XXX dataset, clearly outperform other existing approaches (å½“å‰æœ€å¥½çš„æ•ˆæœ). The code of the paper is made publicity available at XXX (å¼€æºä»£ç ). æœ¬æ–‡çš„ä¸»è¦ç»“æ„ The rest of the paper is organized as follows. In Sec. 2, we describe Ñ…Ñ…Ñ…. XXÑ… are then presented in Sect. 3, and the XXX are presented/reported in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess XXX in Appendix A. 5.4 å®ä¾‹åˆ†æ Face recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source (å®šä¹‰). Face recognition are preliminary steps to a wide range of applications such as personal identity verification [1, 2], video-surveillance [3], lip tracking [4,5], facial expression extraction [6-8], etc (æ„ä¹‰). While there is a large number of studies that have been devoted to the such field, existing approaches generally employed various hand-crafted visual descriptors and generated relatively low and unstable recognition performance on wild datasets (å¤§è‡´æƒ…å†µå’Œç—›ç‚¹). Due to the recent advance in hand-craft features and classifiers, a large number of attempts have been made to face recognition area. In particular, these studies [6-10] can be categorized into three categories: A-based approaches, B-based approaches and C-based approaches (ä¸€å¥è¯æ€»ç»“). The systems proposed by Luke et al.Â [8,9] and Yao et al.Â [10] are the typical examples of A-based approaches, which XXX (ä»‹ç»Lukeå’ŒYaoçš„æ–¹æ³•å’Œæ•ˆæœ). Although A-based approach XXX (åŒ…æ‹¬ç”¨Aæ–¹æ³•çš„å¥½å¤„)ï¼Œthey XXX (åŒ…æ‹¬ç”¨Aæ–¹æ³•çš„ç¼ºç‚¹). Thus, B- based approaches have been widely investigated. For example, Alice et al.Â [7] XXX (ä»‹ç»Aliceçš„æ–¹æ³•å’Œæ•ˆæœ). Besides, Walker et al.Â [6] extends C to face recognition tasks, by XXX (ä»‹ç»Walkerçš„æ–¹æ³•å’Œæ•ˆæœ). While the aforementioned approaches already achieved good recognition performance (around 90% accuracy) [9] on the face dataset that collected under controlled environments [11]ï¼Œthey are still not able to provide reliable predictions on wild datasets [12-14] (ä¸€å¥è¯æ€»ç»“ç°æœ‰æ–¹æ³•ï¼Œä¸€èˆ¬å…ˆæ‰¬åæŠ‘). In addition, XXX (å¦‚æœä¸€å¥ä¸å¤Ÿï¼Œå¯ä»¥å†åŠ ä¸€å¥è¡¥å……ç°æœ‰æ–¹æ³•çš„ç¼ºç‚¹). This is because that existing hand-crafted visual descriptors are usually designed for general computer vision tasks without considering the face-specific information, which is crucial to face recognition tasks (å…·ä½“åˆ†æå½“å‰æ–¹æ³•çš„é—®é¢˜æ‰€åœ¨ã€‚ç”±äºæˆ‘ä»¬æ˜¯æå‡ºç”¨æ·±åº¦ç‰¹å¾æ¥åšäººè„¸è¯†åˆ«ï¼Œè‡ªç„¶æˆ‘ä»¬æ‰€è¦æå‡ºçš„é—®é¢˜å°±è¦é’ˆå¯¹äºå½“å‰çš„æ‰‹å·¥ç‰¹å¾). As a consequence, this paper proposes to extract task- specified descriptors for face recognition, aiming to further enhance the face recognition performance under both controlled and wild conditions. In this paper, we extend Convolution Neural Networks (CNNs) to the face domain (åŸºäºCNNç®—æ³•). Specifically, the proposed approach starts with generating aligned face removing all background noises. Then, it feeds training faces to the proposed CNN model, paired with the corresponding labels. This way, the weights of the utilized CNN would be optimized, in order to correctly recognize the identity of the input face. In other words, the deep learned descriptors, which are generated by the well-trained CNN, are task-specific (æœ¬æ–‡æ–¹æ³•æ­¥éª¤å’ŒåŸç†). To the best of our knowledge, this is the first attempt to extract deep face descriptor for face recognition task (æœ¬æ–‡çš„åˆ›æ–°ç‚¹). To evaluate the performance of the proposed deep learning approach on both controlled and wild conditions, this paper conducts experiments on XXX dataset and XXX dataset (éœ€è¦åšå“ªäº›å®éªŒå’Œå®éªŒç›®çš„). In addition, we also evaluate the learned deep feature on XXX face tasks (å¦‚æœè¿˜æœ‰é™¤ä¸»å®éªŒå¤–çš„å®éªŒï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ). The experimental results show that (ç®€è¦åˆ†æå®éªŒç»“æœ). The main contributions of this study can be summarized as follows: The main contributions of this study can be summarized as follows: This paper proposes the first deep learning-based face recognition system, which learns task-specific face descriptors rather than hand-crafted general visual descriptors (æ–¹æ³•å’Œåˆ›æ–°). The proposed approach achieved the state-of-the-art XXX performance on XXX dataset and XXX dataset, clearly outperform other existing approaches (å½“å‰æœ€å¥½çš„æ•ˆæœ). The code of the paper is made publicity available at XXX (å¼€æºä»£ç ). The rest of the paper is organized as follows. In Sec. 2, we describe Ñ…Ñ…Ñ…. XXÑ… are then presented in Sect. 3, and the XXX are presented/reported in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess XXX in Appendix A. 5.5 æ¨¡ç‰ˆ 5.6 å°å®‹è€å¸ˆå»ºè®® å¤šé˜…è¯»ä¸åŒç±»å‹è®ºæ–‡çš„å¼•è¨€ å¯¹é˜…è¯»è¿‡çš„å¼•è¨€è¿›è¡Œæˆåˆ†åˆ†æ æŠŠåŒä¸€ç§å†…å®¹çš„å¼•è¨€ç”¨ä¸åŒçš„é£æ ¼å†™å‡ºæ¥ "],["abstract-conclusion.html", "Chapter6 æ‘˜è¦å’Œæ€»ç»“ 6.1 æ€ç»´å¯¼å›¾ 6.2 å‡†å¤‡å·¥ä½œ 6.3 æ’°å†™æ‘˜è¦ 6.4 æ’°å†™ç»“è®ºç« èŠ‚", " Chapter6 æ‘˜è¦å’Œæ€»ç»“ 6.1 æ€ç»´å¯¼å›¾ 6.2 å‡†å¤‡å·¥ä½œ å®šä¹‰ æ‘˜è¦å’Œç»“è®ºéƒ¨åˆ†å‡å±äºæ€»ç»“æ¦‚æ‹¬æ€§è´¨çš„ç« èŠ‚ã€‚å› æ­¤ï¼Œåœ¨æ’°å†™A|ç§‘å­¦è®ºæ–‡æ—¶ï¼Œä¸€èˆ¬å®Œæˆå…¨æ–‡å…¶ä»–éƒ¨åˆ†åï¼Œæœ€åå†è¿›è¡Œæ‘˜è¦å’Œç»“è®ºçš„æ’°å†™ã€‚ å®Œæˆå…¨æ–‡å…¶ä»–ç« èŠ‚çš„æ’°å†™ ç¡®å®šå…¨æ–‡æœ€çªå‡º/æœ€æ ¸å¿ƒçš„åˆ›æ–°ç‚¹æˆ–è´¡çŒ®(ä¸€èˆ¬ä¸è¶…è¿‡ä¸‰ä¸ª) æ ¹æ®å®éªŒç»“æœæ€»ç»“å‡ºæœ€é‡è¦/æœ€æ˜¾è‘—çš„ç»“è®º ç®€çŸ­æ€»ç»“å®éªŒç»“æœå’Œç ”ç©¶èƒŒæ™¯ 6.3 æ’°å†™æ‘˜è¦ æ‘˜è¦å››è¦ç´  ç ”ç©¶èƒŒæ™¯/ç—›ç‚¹(1-2å¥): ç—›ç‚¹éœ€è¦æ˜¯æœ¬æ–‡ç ”ç©¶éœ€è¦è§£å†³çš„é—®é¢˜ ç ”ç©¶æ–¹æ³•(2-4å¥): æ–¹æ³•æµç¨‹æˆ–æ ¸å¿ƒç®—æ³• å®éªŒç»“æœ(1-2å¥):å®éªŒè®¾å®šï¼Œå®éªŒç»“æœ è®ºæ–‡ç»“è®º(1å¥):æ€»ç»“ç»“æœï¼Œä»€ä¹ˆæ¡ä»¶ä¸‹ä½ çš„æ–¹æ³•å¥½ 6.4 æ’°å†™ç»“è®ºç« èŠ‚ ç»“è®ºéœ€è¦åŒ…å« ç®€å•çš„æ¦‚è¿°è®ºæ–‡æ–¹æ³•å’Œè§£å†³äº†ä»€ä¹ˆé—®é¢˜âœ”ï¸ ç®€å•ä»‹ç»å®éªŒç»“æœ åˆ†æå®éªŒç»“æœè¯´æ˜äº†ä»€ä¹ˆé—®é¢˜åŠæ‰€æ­ç¤ºçš„æ‰€æå‡ºæ–¹æ³•çš„åŸç†ï¼Œè§„å¾‹å’Œä¼˜åŠ¿âœ”ï¸ æ ¹æ®å®éªŒç»“æœï¼Œæ€»ç»“æ‰€æå‡ºæ–¹æ³•çš„åº”ç”¨ä»·å€¼ æœ¬æ–‡æ–¹æ³•é—ç•™çš„é—®é¢˜åŠä¸è¶³ ä¸æœ¬æ–‡ç›¸å…³æœªæ¥çš„å·¥ä½œè®¡åˆ’ æ¨¡ç‰ˆ ç ”ç©¶èƒŒæ™¯/ç—›ç‚¹(1å¥):ç—›ç‚¹éœ€è¦æ˜¯æœ¬æ–‡ç ”ç©¶éœ€è¦è§£å†³çš„é—®é¢˜ ç ”ç©¶æ–¹æ³•(2å¥):æ–¹æ³•æµç¨‹æˆ–æ ¸å¿ƒç®—æ³• å®éªŒç»“æœ(1-2 å¥):å®éªŒè®¾å®šï¼Œå®éªŒç»“æœ æ ¹æ®å®éªŒç»“æœå¾—åˆ°çš„å„ä¸ªç»“è®º(3-5å¥) æœ¬æ–‡æ–¹æ³•çš„ä¸è¶³å’Œæœªæ¥å·¥ä½œ(2-3å¥) "],["experiments.html", "Chapter7 å®éªŒç»“æœ 7.1 æ€ç»´å¯¼å›¾ 7.2 å‡†å¤‡å·¥ä½œ 7.3 æ’°å†™å®éªŒç« èŠ‚ 7.4 å›é¡¾ä¸æ€»ç»“", " Chapter7 å®éªŒç»“æœ 7.1 æ€ç»´å¯¼å›¾ 7.2 å‡†å¤‡å·¥ä½œ This section should present and discuss the research results, respectively. However, because readers can seldom make sense of results alone without accompanying interpretation they need to be told what the results mean. ç¡®å®šå®éªŒæ•°æ®åº“ æ˜¯å¦æœ‰å…¬å…±æ•°æ®é›† å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸­å“ªäº›æœ€é€‚ç”¨äºæœ¬æ–‡ ä½¿ç”¨å¤šå°‘ä¸ªæ•°æ®é›† æ˜¯å¦æœ‰ç‰¹æ®Šéƒ¨åˆ†éœ€è¦ç”¨å…¶ä»–ä»»åŠ¡è¿›è¡ŒéªŒè¯ ç¡®å®šç±»ä¼¼è®ºæ–‡å®éªŒç« èŠ‚ç»“æ„:ç¡®å®šå¤§è‡´éœ€è¦åšå“ªäº›å®éªŒ ç¡®å®šæ–¹æ³•åˆ›æ–°ç‚¹:ç¡®å®šéœ€è¦é‡ç‚¹åˆ†ææ–¹æ³•å“ªäº›éƒ¨åˆ† æ–¹æ³•ä¸­å„å…ƒç´ é‡è¦æ€§åˆ†æå¯¹æ¯” æœ¬æ–‡æ–¹æ³•ä¸å…¶ä»–è¿‘æœŸæ–¹æ³•å¯¹æ¯” å¯é æ€§/æ”¶æ•›é€Ÿåº¦/æ–¹æ³•ä¸­é—´å˜é‡ç­‰åˆ†æ ç¡®å®šè¯„ä»·æ ‡å‡†å’ŒæŒ‡æ ‡ åˆ†ç±»é—®é¢˜:Recall, Precision, F1, Accuracy ç­‰ å›å½’é—®é¢˜:MAE, MSE, PCC, CCCç­‰ å…¶å®ƒ:Inception Scoreç­‰ å¾—åˆ°æ‰€æœ‰å®éªŒç»“æœ:ç¡®å®šå“ªäº›ç»“æœéœ€è¦æ”¾åœ¨è®ºæ–‡ä¸­ ä¸æ˜¯æ‰€æœ‰çš„ç»“æœéƒ½éœ€è¦æ”¾åœ¨è®ºæ–‡ä¸­ã€‚ æ‰¬é•¿é¿çŸ­:é€‰æ‹©æ”¯æŒè®ºæ–‡ç»“è®ºçš„ç»“æœã€‚ é€‰æ‹©çš„åº•çº¿:å®éªŒç»“æœä¸èƒ½é€ å‡ï¼Œä¸èƒ½é€‰æ‹©é å·§åˆå¾—åˆ°çš„ç»“æœï¼ŒçœŸå®å®éªŒç»“æœå¿…é¡»ä¸è®ºæ–‡ä¸­çš„ç»“è®ºåŒ¹é…ã€‚ ğŸ’¡åœ¨æ’°å†™å®éªŒç« èŠ‚å‰ï¼Œå¿…é¡»è¦å®Œæˆæ‰€æœ‰çš„å®éªŒå¹¶è·å¾—ç»“æœï¼Œè¿™æ ·æ‰å¯ä»¥ç¡®å®šå¥½æœ€ç»ˆéœ€è¦å°†å“ªäº›ç»“æœæ”¾ä¸Šè®ºæ–‡ã€‚åœ¨æ’°å†™å®éªŒç« èŠ‚æ—¶ï¼Œé™¤äº†æ•´ä½“ç³»ç»Ÿä¸ä»–äººæ–¹æ³•å¯¹æ¯”å¤–ï¼Œè¿˜éœ€è¦æ·»åŠ èƒ½çªå‡ºè®ºæ–‡æ–¹æ³•åˆ›æ–°ç‚¹çš„å®éªŒç»“æœã€‚ 7.3 æ’°å†™å®éªŒç« èŠ‚ å®éªŒè®¾å®š å®éªŒç»“æœå¯¹æ¯” å®éªŒåˆ†æ 7.3.1 å®éªŒè®¾å®š æ•°æ®åº“ä»‹ç»:åç§°ï¼Œæ•°æ®å½¢å¼ï¼Œé‡‡é›†æ–¹å¼ï¼Œæ ‡ç­¾ï¼Œæ ·æœ¬é‡ç­‰ã€‚ å®éªŒç»“æœè¡¡é‡æŒ‡æ ‡ å®éªŒæ“ä½œç»†èŠ‚:ä»£ç ä¾èµ–è½¯ç¡¬ä»¶å¹³å°ï¼Œè®­ç»ƒéªŒè¯è®¾å®šï¼Œè¶…å‚æ•°è®¾å®šï¼Œé¢„å¤„ç†ç­‰ã€‚ å…¶ä»–tricks 7.3.2 å®éªŒç»“æœå¯¹æ¯” æœ¬æ–‡æ–¹æ³•å„æ¨¡å—æ€§èƒ½/è´¡çŒ®åˆ†æ(Ablation studies) æœ¬æ–‡æ–¹æ³•ä¸ä»–äººæ–¹æ³•å¯¹æ¯” 7.3.3 å®éªŒåˆ†æ è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ æ”¶æ•›é€Ÿåº¦ å…¶ä»– 7.4 å›é¡¾ä¸æ€»ç»“ æ¨¡ç‰ˆ æ•°æ®åº“ä»‹ç»(Database) æ“ä½œç»†èŠ‚(Implementation details) åº¦é‡æŒ‡æ ‡(Evaluation Metrics) å®éªŒç»“æœ(Experimental Results) Ablation studies Comparison to others å®éªŒç»“æœåˆ†æ(Result analysis/Discussions) "]]
