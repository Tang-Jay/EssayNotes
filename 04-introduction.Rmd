# å¼•è¨€å†™ä½œ {#introduction}
`what why when how`

## æ€ç»´å¯¼å›¾
![](images/Introduction.png)

## å‡†å¤‡å·¥ä½œ
The Introduction section clarifies the motivation for the work presented and prepares readers for the structure of the paper.

**1. æ€»ç»“ç°æœ‰æ–‡çŒ®**

- æ˜ç¡®ç›®å‰æ‰€ç ”ç©¶æ–¹å‘çš„ç°çŠ¶å’ŒèƒŒæ™¯
- ç†è§£ç›®å‰æ‰€ç ”ç©¶æ–¹å‘çš„ä¸»æµæ–¹æ³•
- æ‰¾åˆ°ç›®å‰ä¸»æµæ–¹æ³•çš„é—®é¢˜å’Œç¼ºé™·

**2. æ˜ç¡®ç ”ç©¶åŠ¨æœº**

- ç¡®å®šç ”ç©¶å·¥ä½œçš„é‡å¿ƒ
- ç¡®å®šè®ºæ–‡å†™ä½œé‡å¿ƒ
- ç¡®å®šå®éªŒæ–¹å‘

**3. å®Œæˆç ”ç©¶å†…å®¹**

- æ˜ç¡®è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹
- æ˜ç¡®æœ€ç»ˆä½¿ç”¨çš„æ–¹æ³•å’Œç ”ç©¶ç»†èŠ‚
- ç¡®å®šè®ºæ–‡ä¸­éœ€è¦ä¹¦å†™çš„å†…å®¹

**4. å¾—åˆ°å®éªŒç»“æœ**

- ç¡®å®šè®ºæ–‡å®éªŒå†…å®¹
- æ ¹æ®å®éªŒç»“æœå¾—åˆ°è®ºæ–‡ç»“è®º
- å¯»æ‰¾æŠ•ç¨¿ç›®æ ‡æœŸåˆŠ/ä¼šè®®

ğŸ’¡Introductionæ˜¯æ€»ç»“æ€§çš„ç« èŠ‚ï¼Œå‡ ä¹éœ€è¦åŒ…å«ç ”ç©¶å·¥ä½œçš„æ‰€æœ‰å†…å®¹ã€‚å› æ­¤åœ¨æ’°å†™å‰å»ºè®®å°†æ€»ç»“å¥½çš„æ–‡çŒ®å®Œæˆçš„ç ”ç©¶å†…å®¹å’Œå®éªŒç»“æœéƒ½ä»”ç»†å›é¡¾å‡ éï¼ŒåŠ æ·±å°è±¡ã€‚åœ¨æ’°å†™æ—¶ï¼Œå»ºè®®å°†è¿™äº›èµ„æ–™æ‘†åœ¨ä¸€æ—ï¼Œä»¥ä¾¿éšæ—¶æŸ¥é˜…ã€‚

## æ’°å†™å¼•è¨€
### åŸºæœ¬ç»“æ„
![](images/Introduction2.png)

### ç ”ç©¶èƒŒæ™¯å’Œç—›ç‚¹

- ç¬¬ä¸€å¥ï¼Œç ”ç©¶å¤§æ–¹å‘çš„å®šä¹‰
- ç¬¬äºŒå¥ï¼Œç ”ç©¶æ–¹å‘æ„ä¹‰
- ç¬¬ä¸‰å¥ï¼Œç ”ç©¶æ–¹å‘å½“å‰çš„å¤§è‡´ç°çŠ¶
- ç¬¬å››å¥ï¼Œç—›ç‚¹ï¼ˆä¸æˆ‘ä»¬ç ”ç©¶å†…å®¹ç›¸å…³çš„ï¼Œæ–¹ä¾¿å¼•å‡ºä¸‹ä¸€æ®µï¼‰

### ç°æœ‰æ–¹æ³•çš„æ€»ç»“å½’çº³

**1. èµ·å§‹å¥:å¼•å‡ºç°æœ‰æ–¹æ³•**

ç”¨ä¸€å¥è¯å¼•å‡ºå½“å‰å·²ç»æå‡ºçš„å±äºæœ¬æ–‡ç ”ç©¶é¢†åŸŸçš„æ–¹æ³•:
  - (With XXX becoming XXX/Due to the recent advance in Ñ…Ñ…Ñ…), (a large number of attempts have been made/ a number of similar studies have been conducted) to XXX
  - XXX can be categorized into three fields: XXX, XXX and XXX
  - XXX is/are/becomes very popular in XXX field since XXX

**2. æ¦‚è¿°ç›®å‰å·²æœ‰çš„ç»å…¸å·¥ä½œ**

- æ—¶åºæ³•:æ ¹æ®å·²æœ‰æ–¹æ³•æå‡ºçš„æ—¶é—´å…ˆåé¡ºåºæˆ–å‘å±•é¡ºåºè¿›è¡Œé˜è¿°ã€‚
- åˆ†ç±»æ³•:æ ¹æ®å·²æœ‰æ–¹æ³•æ‰€å±çš„ç±»åˆ«(å¦‚æ‰‹å·¥ç‰¹å¾/æ·±åº¦ç‰¹å¾ï¼Œä½¿ç”¨æ—¶åºä¿¡
æ¯/æœªä½¿ç”¨æ—¶åºä¿¡æ¯ç­‰)åˆ†åˆ«é˜è¿°ã€‚

ğŸ’¡åœ¨æ€»ç»“å„å·¥ä½œæ—¶ï¼Œ-èˆ¬åªéœ€è¦ç”¨1-2å¥è¯ï¼Œä¸å®œå¤ªé•¿(ä¸ç„¶ä¼šä¸Related Worké‡Œå†…å®¹é‡å¤)ã€‚æ€»ç»“æ—¶ï¼Œéœ€æ ¹æ®è®ºæ–‡çš„ç ”ç©¶å†…å®¹ï¼Œæ¦‚è¿°å„å·¥ä½œçš„ä¸»è¦(ç›¸å…³)æ–¹æ³•å’Œä¼˜ç¼ºç‚¹ã€‚

**æ—¶åºæ³•ä¸¾ä¾‹**

In particular, most of these studies [6- 10] firstly extract hand-crafted visual descriptors from face images and then feed them to pre-trained classifiers for prediction (ä¸€å¥è¯æ€»ç»“). Very early studies generally XXX (ç”¨XXXæ–¹æ³•) to obtain face representations, which XXX
(è¿™äº›æ–¹æ³•çš„ç¼ºç‚¹). To solve these problems, Walker et al. [6] proposed a XXX method that (ä»‹ç»Walkerçš„æ–¹æ³•å’Œæ•ˆæœ). Alice et al. [7] XXX (ä»‹ç»Aliceçš„æ–¹æ³•å’Œæ•ˆæœ). However, both approaches XXX (1ä¸ªç»Walkerå’ŒAliceæ–¹æ³•çš„å±€é™æ€§). As a result, Luke et al. [8,9] and Yao et al. [10] specifically investigated XXX and (ä»‹ç»è¿‘æœŸæ›´å…ˆè¿›çš„æ–¹æ³•åŠå…¶å¦‚ä½•è§£å†³ä¹‹å‰æ–¹æ³•çš„é—®é¢˜).

**åˆ†ç±»æ³•ä¸¾ä¾‹**

In particular, these studies [6-10] can be categorized into three categories: A-based approaches, B based approaches and C-based approaches (ä¸€å¥è¯æ€»ç»“). The systems proposed by Luke et al. [8,9]
and Yao et al. [10] are the typical examples of A-based approaches, which XXX (ä»‹ç»Lukeå’ŒYaoçš„æ–¹æ³•å’Œæ•ˆæœ). Although A-based approach
XXX (åŒ…æ‹¬ç”¨Aæ–¹æ³•çš„å¥½å¤„), they XXX (åŒ…æ‹¬ç”¨Aæ–¹æ³•çš„ç¼ºç‚¹). Thus, B-based approaches have been widely investigated. For example, Alice et al. [7] XXX
(ä»‹ç»Aliceçš„æ–¹æ³•å’Œæ•ˆæœ). Besides, Walker et al. [6]
extends C to face recognition tasks, by XXX (ä»‹ç»Walkerçš„æ–¹æ³•å’Œæ•ˆæœ)

**3. æ€»ç»“ç›®å‰å·²æœ‰çš„ç»å…¸å·¥ä½œæ‰€å­˜åœ¨çš„é—®é¢˜**

ä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸€éƒ¨åˆ†éœ€è¦æ€»ç»“ä¸æœ¬æ–‡å†…å®¹ç›¸å…³çš„é—®é¢˜ï¼Œå¹¶ä»¥æ­¤å¼•å‡ºæœ¬æ–‡çš„
Motivation.
ä»¥äººè„¸è¯†åˆ«ä¸ºä¾‹:
å¦‚æœä½ çš„è®ºæ–‡è®¾è®¡äº†ä¸€ç§åœ¨è¯†åˆ«ç‡æ›´é«˜ä½†é€Ÿåº¦å¾ˆæ…¢çš„æ–¹æ³•ï¼Œé‚£ä¹ˆæ­¤å¤„å°±è¦æ€»ç»“ç°æœ‰æ–¹æ³•å‡†ç¡®ç‡ä¸è¶³çš„é—®é¢˜å’Œå¯èƒ½ä¼šå¯¼è‡´çš„åæœã€‚å¦‚æœä½ çš„è®ºæ–‡è®¾è®¡äº†ä¸€ç§é€Ÿåº¦å¾ˆå¿«ä½†å‡†ç¡®åº¦æ— æ˜æ˜¾æå‡çš„æ–¹æ³•ï¼Œé‚£ä¹ˆè®ºæ–‡ä¸­å°±è¦æ€»ç»“ç°æœ‰æ–¹æ³•é€Ÿåº¦ä¸è¶³çš„é—®é¢˜å’Œå¯èƒ½ä¼šå¯¼è‡´çš„åæœã€‚

**ä¸¾ä¾‹**

While the aforementioned approaches already achieved good recognition performance (around 90% accuracy) [9] on the face dataset that collected under controlled environments [11]ï¼Œthey are still not able to provide reliable predictions on wild datasets [12-14]
(ä¸€å¥è¯æ€»ç»“ç°æœ‰æ–¹æ³•ï¼Œä¸€èˆ¬å…ˆæ‰¬åæŠ‘). In addition, XXX (å¦‚æœä¸€å¥ä¸å¤Ÿï¼Œå¯ä»¥å†åŠ ä¸€å¥è¡¥å……ç°æœ‰æ–¹æ³•çš„ç¼ºç‚¹). This is because that existing hand-crafted visual descriptors are usually designed for general computer vision tasks without considering the face-specific information, which is crucial to face recognition tasks (å…·ä½“åˆ†æå½“å‰æ–¹æ³•çš„é—®é¢˜æ‰€åœ¨ã€‚ç”±äºæˆ‘ä»¬æ˜¯æå‡ºç”¨æ·±åº¦ç‰¹å¾æ¥åšäººè„¸è¯†åˆ«ï¼Œè‡ªç„¶æˆ‘ä»¬æ‰€è¦æå‡ºçš„é—®é¢˜å°±è¦é’ˆå¯¹äºå½“å‰çš„æ‰‹å·¥ç‰¹å¾).

### æœ¬æ–‡ç ”ç©¶æ¦‚è¿°

**æœ¬æ–‡ç ”ç©¶ç»¼è¿°æ®µè½åŒ…å«äº†ç ”ç©¶ç›®çš„ï¼Œç ”ç©¶æ–¹æ³•å’Œå®éªŒè®¾è®¡ã€‚ï¼ˆå‘¼åº”ä¸Šä¸€æ®µæœ€åä¸€å¥è¯ï¼‰**

1. æ ¹æ®ä¸Šæ®µæœ€åæ€»ç»“çš„ç°æœ‰æ–¹æ³•çš„ä¸»è¦é—®é¢˜ï¼Œæå‡ºæœ¬æ–‡çš„ç ”ç©¶ç›®çš„ã€‚
- To address/solve/deal with XXX, this paper presents/proposes XXX
- In this paper, we aims to XXX by XXX
- As a consequence, this paper XXX

2. æå‡ºå…·ä½“çš„è§£å†³æ–¹æ¡ˆã€‚
- åŸºäºå“ªäº›å·²æœ‰çš„ç®—æ³•
- æå‡ºæ–¹æ³•çš„ä¸»è¦æµç¨‹åŠåŸç†
- æå‡ºæ–¹æ³•çš„åˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆ

**ä¸¾ä¾‹**

In this paper, we extend Convolution Neural Networks (CNNs) to the face domain (åŸºäºCNNç®—æ³•). Specifically, the proposed approach starts with generating aligned face removing all background noises. Then, it feeds training faces to the proposed CNN model, paired with the corresponding labels. This way, the weights of the utilized CNN would be optimized, in order to correctly recognize the identity of the input face. In other words, the deep learned descriptors, which are generated by the well-trained CNN, are task -specific (æœ¬æ–‡æ–¹æ³•æ­¥éª¤å’ŒåŸç†). To the best of our knowledge, this is the first attempt to extract deep face descriptor for face recognition task (æœ¬æ–‡çš„åˆ›æ–°ç‚¹).

3. æå‡ºéªŒè¯æ–¹æ¡ˆã€‚
- éœ€è¦åšå“ªäº›å®éªŒ
- å®éªŒç›®çš„æ˜¯ä»€ä¹ˆ
- ç®€ç•¥å™è¿°å®éªŒç»“æœ

**ä¸¾ä¾‹**

To evaluate the performance of the proposed deep learning approach on both controlled and wild conditions, this paper conducts experiments on XXX dataset and XXX dataset (éœ€è¦åšå“ªäº›å®éªŒå’Œå®éªŒç›®çš„). In addition, we also evaluate the learned deep feature
on XXX face tasks (å¦‚æœè¿˜æœ‰é™¤ä¸»å®éªŒå¤–çš„å®éªŒï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ). The
experimental results show that (ç®€è¦åˆ†æå®éªŒç»“æœ).

4. æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®
- æ–¹æ³•çš„åˆ›æ–°ç‚¹
- å®éªŒç»“æœçš„å¤§å¹…æå‡
- å¼€æºä»£ç 
- å¼€æºæ•°æ®åº“

### ç»“å°¾:æœ¬æ–‡è´¡çŒ®åŠè®ºæ–‡ç»“æ„

1. æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®

The main contributions of this study can be summarized as follows:
This paper proposes the first deep learning-based face recognition system, which learns task-specific face descriptors rather than hand-crafted general visual descriptors (æ–¹æ³•å’Œåˆ›æ–°).
The proposed approach achieved the state-of-the-art XXX performance on XXX dataset and XXX dataset, clearly outperform other existing approaches (å½“å‰æœ€å¥½çš„æ•ˆæœ).
The code of the paper is made publicity available at XXX (å¼€æºä»£ç ).

2. æœ¬æ–‡çš„ä¸»è¦ç»“æ„

The rest of the paper is organized as follows. In Sec. 2, we describe Ñ…Ñ…Ñ…. XXÑ… are then presented in Sect. 3, and the XXX are
presented/reported in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess XXX in Appendix A.

## å®ä¾‹åˆ†æ

Face recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source (å®šä¹‰). Face recognition are preliminary steps to a wide range of applications such as personal identity verification [1, 2], video-surveillance [3], lip tracking
[4,5], facial expression extraction [6-8], etc (æ„ä¹‰). While there is a large number of studies that have been devoted to the such field, existing approaches generally employed various hand-crafted visual
descriptors and generated relatively low and unstable recognition performance on wild datasets (å¤§è‡´æƒ…å†µå’Œç—›ç‚¹).

Due to the recent advance in hand-craft features and classifiers, a large number of attempts have been made to face recognition area. In particular, these studies [6-10] can be categorized into three categories: A-based approaches, B-based approaches and C-based approaches (ä¸€å¥è¯æ€»ç»“). The
systems proposed by Luke et al. [8,9] and Yao et al. [10] are the typical examples of A-based
approaches, which XXX (ä»‹ç»Lukeå’ŒYaoçš„æ–¹æ³•å’Œæ•ˆæœ). Although A-based approach XXX (åŒ…æ‹¬ç”¨Aæ–¹æ³•çš„å¥½å¤„)ï¼Œthey XXX (åŒ…æ‹¬ç”¨Aæ–¹æ³•çš„ç¼ºç‚¹). Thus, B- based approaches have been widely investigated. For
example, Alice et al. [7] XXX (ä»‹ç»Aliceçš„æ–¹æ³•å’Œæ•ˆæœ). Besides, Walker et al. [6] extends C to face recognition tasks, by XXX (ä»‹ç»Walkerçš„æ–¹æ³•å’Œæ•ˆæœ).

While the aforementioned approaches already achieved good recognition performance (around 90% accuracy) [9] on the face dataset that collected under controlled environments [11]ï¼Œthey are still not able to provide reliable predictions on wild datasets [12-14]
(ä¸€å¥è¯æ€»ç»“ç°æœ‰æ–¹æ³•ï¼Œä¸€èˆ¬å…ˆæ‰¬åæŠ‘). In addition, XXX (å¦‚æœä¸€å¥ä¸å¤Ÿï¼Œå¯ä»¥å†åŠ ä¸€å¥è¡¥å……ç°æœ‰æ–¹æ³•çš„ç¼ºç‚¹). This is because that existing hand-crafted visual descriptors are usually designed for general computer vision tasks without considering the face-specific information, which is crucial to face recognition tasks (å…·ä½“åˆ†æå½“å‰æ–¹æ³•çš„é—®é¢˜æ‰€åœ¨ã€‚ç”±äºæˆ‘ä»¬æ˜¯æå‡ºç”¨æ·±åº¦ç‰¹å¾æ¥åšäººè„¸è¯†åˆ«ï¼Œè‡ªç„¶æˆ‘ä»¬æ‰€è¦æå‡ºçš„é—®é¢˜å°±è¦é’ˆå¯¹äºå½“å‰çš„æ‰‹å·¥ç‰¹å¾).

As a consequence, this paper proposes to extract task- specified descriptors for face recognition, aiming to further enhance the face recognition performance under both controlled and wild conditions. In this paper, we extend Convolution Neural Networks (CNNs) to the face domain (åŸºäºCNNç®—æ³•). Specifically, the proposed approach starts with generating aligned face removing all background noises. Then, it feeds training faces to the proposed CNN model, paired with the corresponding labels. This way, the weights of the utilized CNN would be optimized, in order to correctly recognize the identity of the input face. In other words, the deep learned descriptors, which are generated by the well-trained CNN, are task-specific (æœ¬æ–‡æ–¹æ³•æ­¥éª¤å’ŒåŸç†). To the best of our knowledge, this is the first attempt to extract deep face descriptor for face recognition task (æœ¬æ–‡çš„åˆ›æ–°ç‚¹).

To evaluate the performance of the proposed deep learning approach on both controlled and wild conditions, this paper conducts experiments on XXX dataset and XXX dataset (éœ€è¦åšå“ªäº›å®éªŒå’Œå®éªŒç›®çš„). In addition, we also evaluate the learned deep feature on XXX face tasks (å¦‚æœè¿˜æœ‰é™¤ä¸»å®éªŒå¤–çš„å®éªŒï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ). The experimental results show that (ç®€è¦åˆ†æå®éªŒç»“æœ). The main contributions of this study can be summarized as follows:
The main contributions of this study can be summarized as follows:

1. This paper proposes the first deep learning-based face recognition system, which learns task-specific face descriptors rather than hand-crafted general visual descriptors (æ–¹æ³•å’Œåˆ›æ–°).

1. The proposed approach achieved the state-of-the-art XXX performance on XXX dataset and XXX dataset, clearly outperform other existing approaches (å½“å‰æœ€å¥½çš„æ•ˆæœ).

1. The code of the paper is made publicity available at XXX (å¼€æºä»£ç ).

The rest of the paper is organized as follows. In Sec. 2, we describe Ñ…Ñ…Ñ…. XXÑ… are then presented in Sect. 3, and the XXX are
presented/reported in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess XXX in Appendix A.

## æ¨¡ç‰ˆ
![](images/1.png)
![](images/2.png)
![](images/3.png)
![](images/4.png)
![](images/5.png)
![](images/6.png)


## å°å®‹è€å¸ˆå»ºè®®

- å¤šé˜…è¯»ä¸åŒç±»å‹è®ºæ–‡çš„å¼•è¨€
- å¯¹é˜…è¯»è¿‡çš„å¼•è¨€è¿›è¡Œæˆåˆ†åˆ†æ
- æŠŠåŒä¸€ç§å†…å®¹çš„å¼•è¨€ç”¨ä¸åŒçš„é£æ ¼å†™å‡ºæ¥


